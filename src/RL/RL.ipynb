{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.验证模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from base_module import *\n",
    "\n",
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# 检测GPU是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# 测试用输入\n",
    "batch =  []\n",
    "batch_size = 16\n",
    "feature_channels = 35\n",
    "feature_height = 4\n",
    "feature_width = 9\n",
    "\n",
    "action_space = 47\n",
    "\n",
    "while len(batch) < batch_size:\n",
    "    while not env.is_over():\n",
    "        curr_pid = env.get_curr_player_id()\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        action = np.random.choice(valid_actions)\n",
    "        env.step(player_id=curr_pid, action=action)\n",
    "    print(env.get_payoffs())\n",
    "    for i in range(4):\n",
    "        batch.append(env.get_observation_with_return(i))\n",
    "    env.reset()\n",
    "collator = myCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_module import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "\n",
    "# 检测GPU是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "# 创建模型\n",
    "config = GPT2Config(n_embd=512, n_layer=8, n_head=8, n_positions=128)\n",
    "model = Policy_Network(config).to(device)\n",
    "data = collator(batch)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = model.forward(**data)\n",
    "print(\"action_logits:\", output[\"action_logits\"].shape)\n",
    "print(output[\"action_logits\"][0])\n",
    "print(\"probs:\", output[\"action_probs\"][0])\n",
    "# print(\"loss:\", output[\"loss\"])\n",
    "print(\"action:\", output[\"action\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.验证麻将环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.get_valid_actions())\n",
    "print(env.legal_actions_mask_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not env.is_over():\n",
    "    curr_pid = env.get_curr_player_id()\n",
    "    valid_actions = env.get_valid_actions()\n",
    "    action = np.random.choice(valid_actions)\n",
    "    env.step(player_id=curr_pid, action=action)\n",
    "print(env.get_payoffs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 展示当前phase和合法动作列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase, aviable_action = env._proceed()\n",
    "print(\"phase:\", phase)\n",
    "for idx, action in enumerate(aviable_action):\n",
    "    print(idx, action.to_string())\n",
    "\n",
    "valid_action = env.get_valid_actions()\n",
    "print(\"valid_action:\", valid_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 执行动作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(\"player\", i, \"hand:\", env._get_hand_tiles(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.get_observation(1)\n",
    "print(\"tiles_features shape:\", obs['tiles_features'].shape)\n",
    "print(\"oya shape:\", obs['info']['oya'].shape)\n",
    "print(\"riichi_sticks shape:\", obs['info']['riichi_sticks'].shape)\n",
    "print(\"action_list shape:\", obs['action_list'].shape)\n",
    "print(\"action_list:\", obs['action_list'])\n",
    "print(\"self_action_mask shape:\", obs['self_action_mask'].shape)\n",
    "print(\"self_action_mask:\", obs['self_action_mask'])\n",
    "print(\"sum self_action_mask:\", obs['self_action_mask'].sum())\n",
    "print(\"attention_mask shape:\", obs['attention_mask'].shape)\n",
    "print(\"attention_mask:\", obs['attention_mask'])\n",
    "print(\"Q shape:\", obs['Q_values'].shape)\n",
    "print(\"Q:\", obs['Q_values'])\n",
    "print(\"legal_action_mask shape:\", obs['legal_action_mask'].shape)\n",
    "print(\"legal_action_mask:\", obs['legal_action_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(\"player\", i, \"fuuros:\", env._get_fuuros(i))\n",
    "    print(\"points:\", env._get_points(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs['info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.使用模型来决策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "\n",
    "# 使用第一个可用的 GPU，即设备 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from base_module import *\n",
    "\n",
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "env.reset()\n",
    "\n",
    "config = GPT2Config(n_embd=512, n_layer=8, n_head=8, n_positions=128)\n",
    "model = Policy_Network(config).to(device)\n",
    "model.eval()\n",
    "\n",
    "collator = myCollator()\n",
    "for i in range(100):\n",
    "    while not env.is_over():\n",
    "        curr_pid = env.get_curr_player_id()\n",
    "        obs = env.get_observation(curr_pid)\n",
    "        input = {\n",
    "        \"tiles_features\": torch.tensor(obs['tiles_features'], dtype=torch.float32).to(device),\n",
    "        \"oya\": torch.tensor(obs['info']['oya'], dtype=torch.float32).unsqueeze(0).to(device),\n",
    "        \"riichi_sticks\": torch.tensor(obs['info']['riichi_sticks'],dtype=torch.float32).unsqueeze(0).to(device),\n",
    "        \"action_list\": torch.tensor(obs['action_list'],dtype=torch.long).unsqueeze(0).to(device),\n",
    "        \"attention_mask\": torch.tensor(obs['attention_mask'],dtype=torch.long).unsqueeze(0).to(device),\n",
    "        \"legal_action_mask\": torch.tensor(obs['legal_action_mask'], dtype=bool).to(device)\n",
    "        }\n",
    "        output = model.inference(**input)\n",
    "        action = output[\"action\"].item()\n",
    "        env.step(player_id=curr_pid, action=action)\n",
    "        # print(\"player:\", curr_pid, \"action:\", action)\n",
    "    print(env.get_payoffs())\n",
    "    env.reset()\n",
    "\n",
    "\n",
    "\n",
    "# print(\"action:\", output[\"action\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import torch\n",
    "\n",
    "# 使用第一个可用的 GPU，即设备 1\n",
    "inference_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "training_device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"inference_device:\", inference_device)\n",
    "print(\"training_device:\", training_device)\n",
    "\n",
    "import numpy as np\n",
    "from base_module import *\n",
    "\n",
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "env.reset()\n",
    "\n",
    "config = GPT2Config(n_embd=512, n_layer=8, n_head=8, n_positions=128)\n",
    "inference_model = Policy_Network(config).to(inference_device)\n",
    "inference_model.eval()\n",
    "\n",
    "training_model = Policy_Network(config).to(training_device)\n",
    "training_model.load_state_dict(inference_model.state_dict())\n",
    "training_model.train()\n",
    "\n",
    "collator = myCollator(device=training_device)\n",
    "optimizer = torch.optim.Adam(training_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "num_games = 10\n",
    "for i in range(epoch): \n",
    "   # 生成10局对局数据\n",
    "    dataset = []\n",
    "    for i in range(num_games):\n",
    "        while not env.is_over():\n",
    "            curr_pid = env.get_curr_player_id()\n",
    "            obs = env.get_observation(curr_pid)\n",
    "            input = {\n",
    "            \"tiles_features\": torch.tensor(obs['tiles_features'], dtype=torch.float32).to(inference_device),\n",
    "            \"oya\": torch.tensor(obs['info']['oya'], dtype=torch.float32).unsqueeze(0).to(inference_device),\n",
    "            \"riichi_sticks\": torch.tensor(obs['info']['riichi_sticks'],dtype=torch.float32).unsqueeze(0).to(inference_device),\n",
    "            \"action_list\": torch.tensor(obs['action_list'],dtype=torch.long).unsqueeze(0).to(inference_device),\n",
    "            \"attention_mask\": torch.tensor(obs['attention_mask'],dtype=torch.long).unsqueeze(0).to(inference_device),\n",
    "            \"legal_action_mask\": torch.tensor(obs['legal_action_mask'], dtype=bool).to(inference_device)\n",
    "            }\n",
    "            output = inference_model.inference(**input)\n",
    "            action = output[\"action\"].item()\n",
    "            env.step(player_id=curr_pid, action=action)\n",
    "        for i in range(4):\n",
    "            dataset.append(env.get_observation_with_return(i))\n",
    "        print(env.get_payoffs())\n",
    "        env.reset()\n",
    "\n",
    "    # 从dataset中每次取出batch_size个数据\n",
    "    batch_size = 4\n",
    "    collator = myCollator()\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        data = collator(batch)\n",
    "        # 移动数据到training_device\n",
    "        for key in data.keys():\n",
    "            if key == \"info\":\n",
    "                for sub_key in data[key].keys():\n",
    "                    data[key][sub_key] = data[key][sub_key].to(training_device)\n",
    "            else:\n",
    "                data[key] = data[key].to(training_device)\n",
    "        output = training_model.forward(**data)\n",
    "        loss = output[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"loss:\", loss.item())\n",
    "\n",
    "    # 将training_model的参数复制到inference_model,并保存checkpoint\n",
    "    inference_model.load_state_dict(training_model.state_dict())\n",
    "    torch.save(inference_model.state_dict(), f\"./checkpoints/epoch_{epoch}.pth\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"phase:\", env.t.get_phase())\n",
    "print(\"player:\", env.get_curr_player_id())\n",
    "print(\"legal actions:\", env.get_valid_actions())\n",
    "print(\"action\", action)\n",
    "action_container = env.act_container\n",
    "nonzero_index = [i for i in range(len(action_container)) if action_container[i] != 0]\n",
    "print(\"nonzero_index:\", nonzero_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(3, 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.检查环境bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/RL/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[ 1500.  1500. -1500. -1500.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[-1000.  3000. -1000. -1000.]\n",
      "[-1000.  3000. -1000. -1000.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[ 3000. -1000. -1000. -1000.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[-1000.  3000. -1000. -1000.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[-1000. -1000. -1000.  3000.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "phase: 3\n",
      "aviable_action: {'Discard', 'Riichi'}\n",
      "[-25000. -25000. -25000. -25000.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from base_module import *\n",
    "\n",
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "\n",
    "flag = True\n",
    "# for i in range(10000):\n",
    "while flag:\n",
    "    env.reset()\n",
    "    while not env.is_over():\n",
    "        curr_pid = env.get_curr_player_id()\n",
    "        phase = env.t.get_phase()\n",
    "        if phase < 4:\n",
    "            aviable_action = env.t.get_self_actions()\n",
    "        elif phase < 16:\n",
    "            aviable_action = env.t.get_response_actions()\n",
    "\n",
    "\n",
    "        ls = []\n",
    "        for action in aviable_action:\n",
    "            ls.append(action.action.name)\n",
    "        set_ls = set(ls)\n",
    "        if \"Riichi\" in set_ls:\n",
    "            flag = False\n",
    "            print(\"phase:\", phase)\n",
    "            print(\"aviable_action:\", set_ls)\n",
    "            break\n",
    "\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        action = np.random.choice(valid_actions)\n",
    "        env.step(player_id=curr_pid, action_idx=action)\n",
    "    print(env.get_payoffs())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/RL/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from base_module import *\n",
    "\n",
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: 0\n",
      "curr_pid: 0\n",
      "aviable_action:\n",
      "0 Discard 9m\n",
      "1 Discard 3p\n",
      "2 Discard 8s\n",
      "3 Discard 2z\n",
      "4 Discard 7z\n"
     ]
    }
   ],
   "source": [
    "print(\"phase:\", env.t.get_phase())\n",
    "print(\"curr_pid:\", env.get_curr_player_id())\n",
    "phase = env.t.get_phase()\n",
    "if phase < 4:\n",
    "    aviable_action = env.t.get_self_actions()\n",
    "elif phase < 16:\n",
    "    aviable_action = env.t.get_response_actions()\n",
    "print(\"aviable_action:\")\n",
    "ls = []\n",
    "for idx, action in enumerate(aviable_action):\n",
    "    ls.append(action.to_string())\n",
    "    print(idx, action.to_string())\n",
    "    # print(action.action.name)\n",
    "\n",
    "# print(\"aviable_action:\", ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33, 8, 11, 25, 28]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_valid_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(env.get_curr_player_id(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.riichi_stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after phase: 4\n",
      "after aviable_action:\n",
      "0 Pass\n"
     ]
    }
   ],
   "source": [
    "env.t.make_selection(7)\n",
    "phase = env.t.get_phase()\n",
    "if phase < 4:\n",
    "    aviable_action = env.t.get_self_actions()\n",
    "elif phase < 16:\n",
    "    aviable_action = env.t.get_response_actions()\n",
    "print(\"after phase:\", phase)\n",
    "print(\"after aviable_action:\")\n",
    "for idx, action in enumerate(aviable_action):\n",
    "    print(idx, action.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after phase: 16\n",
      "after aviable_action:\n",
      "0 Pass\n"
     ]
    }
   ],
   "source": [
    "env.t.make_selection(0)\n",
    "phase = env.t.get_phase()\n",
    "if phase < 4:\n",
    "    aviable_action = env.t.get_self_actions()\n",
    "elif phase < 16:\n",
    "    aviable_action = env.t.get_response_actions()\n",
    "print(\"after phase:\", phase)\n",
    "print(\"after aviable_action:\")\n",
    "for idx, action in enumerate(aviable_action):\n",
    "    print(idx, action.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Yama: 3m 3z 1m 6z 3p 4s 5z 4m 7p 2z 3m 6z 7s 4p (Wanpai)| 8s 3s 8s 7m 5s 5z 3z 8m 3z 7p 0m 5z 8p 6m 3m 2p 1z 1p 7m 4p 6s 4z 5p 4s 5p 9s 5m 2p 3s 3s 5m 6m 4m 6s 6p 3p 4m 1p 8m 6p 3s 5m \n",
      "Dora Indicator(s):4s \n",
      "Remaining tiles: 42\n",
      "Player 0: \n",
      "Pt: 25000\n",
      "Wind: South\n",
      "Hand: 9m 3p 8s 7z 2z \n",
      "Calls: 6p7p(8p) 2m2m(2m) 1m(1m)1m \n",
      "River: 6m2h 9m6h 3z10h 0p14h 8m16h 8p19h 5s23h- 6s26h 4z30h- \n",
      "Riichi: No\n",
      "Menzen: No\n",
      "Player 1: \n",
      "Pt: 25000\n",
      "Wind: West\n",
      "Hand: 6m 2p 8p 1s 1s 2s 4s 6s 7s 2z 2z 5z 7z \n",
      "River: 7s3h 7p7h 1z11h 2m15h- 3m17 7m20h 1p27h \n",
      "Riichi: No\n",
      "Menzen: Yes\n",
      "Player 2: \n",
      "Pt: 25000\n",
      "Wind: North\n",
      "Hand: 7m 8m 2p 9p 9p 8s 7z \n",
      "Calls: 0s(5s)5s 4z(4z)4z \n",
      "River: 7s4h 6p8h 7z12 1m18h- 4s21h 6z24h 2s28h 6z31h \n",
      "Riichi: No\n",
      "Menzen: No\n",
      "Player 3: \n",
      "Pt: 25000\n",
      "Wind: East\n",
      "Hand: 9m 9m 4p 4p 5p 9p 9p 1s 1s 2s 2s 9s 9s \n",
      "River: 1z1h 8p5h- 2m9h 1p13h 9s22h 1z25h 3p29h 4m32h \n",
      "Riichi: No\n",
      "Menzen: Yes\n",
      "\n",
      "Oya player 3\n",
      "Honba: 0\n",
      "Kyoutaku: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(phase)\n",
    "print(env.t.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -700.,  2700., -1300.,  -700.], dtype=float32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_payoffs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
