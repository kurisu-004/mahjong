{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.验证模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from base_module import *\n",
    "\n",
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# 检测GPU是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# 测试用输入\n",
    "batch =  []\n",
    "batch_size = 16\n",
    "feature_channels = 35\n",
    "feature_height = 4\n",
    "feature_width = 9\n",
    "\n",
    "action_space = 47\n",
    "\n",
    "while len(batch) < batch_size:\n",
    "    while not env.is_over():\n",
    "        curr_pid = env.get_curr_player_id()\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        action = np.random.choice(valid_actions)\n",
    "        env.step(player_id=curr_pid, action=action)\n",
    "    print(env.get_payoffs())\n",
    "    for i in range(4):\n",
    "        batch.append(env.get_observation_with_return(i))\n",
    "    env.reset()\n",
    "collator = myCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_module import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "\n",
    "# 检测GPU是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "# 创建模型\n",
    "config = GPT2Config(n_embd=512, n_layer=8, n_head=8, n_positions=128)\n",
    "model = Policy_Network(config).to(device)\n",
    "data = collator(batch)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = model.forward(**data)\n",
    "print(\"action_logits:\", output[\"action_logits\"].shape)\n",
    "print(output[\"action_logits\"][0])\n",
    "print(\"probs:\", output[\"action_probs\"][0])\n",
    "# print(\"loss:\", output[\"loss\"])\n",
    "print(\"action:\", output[\"action\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.验证麻将环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.get_valid_actions())\n",
    "print(env.legal_actions_mask_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not env.is_over():\n",
    "    curr_pid = env.get_curr_player_id()\n",
    "    valid_actions = env.get_valid_actions()\n",
    "    action = np.random.choice(valid_actions)\n",
    "    env.step(player_id=curr_pid, action=action)\n",
    "print(env.get_payoffs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 展示当前phase和合法动作列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase, aviable_action = env._proceed()\n",
    "print(\"phase:\", phase)\n",
    "for idx, action in enumerate(aviable_action):\n",
    "    print(idx, action.to_string())\n",
    "\n",
    "valid_action = env.get_valid_actions()\n",
    "print(\"valid_action:\", valid_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 执行动作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(\"player\", i, \"hand:\", env._get_hand_tiles(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.get_observation(1)\n",
    "print(\"tiles_features shape:\", obs['tiles_features'].shape)\n",
    "print(\"oya shape:\", obs['info']['oya'].shape)\n",
    "print(\"riichi_sticks shape:\", obs['info']['riichi_sticks'].shape)\n",
    "print(\"action_list shape:\", obs['action_list'].shape)\n",
    "print(\"action_list:\", obs['action_list'])\n",
    "print(\"self_action_mask shape:\", obs['self_action_mask'].shape)\n",
    "print(\"self_action_mask:\", obs['self_action_mask'])\n",
    "print(\"sum self_action_mask:\", obs['self_action_mask'].sum())\n",
    "print(\"attention_mask shape:\", obs['attention_mask'].shape)\n",
    "print(\"attention_mask:\", obs['attention_mask'])\n",
    "print(\"Q shape:\", obs['Q_values'].shape)\n",
    "print(\"Q:\", obs['Q_values'])\n",
    "print(\"legal_action_mask shape:\", obs['legal_action_mask'].shape)\n",
    "print(\"legal_action_mask:\", obs['legal_action_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(\"player\", i, \"fuuros:\", env._get_fuuros(i))\n",
    "    print(\"points:\", env._get_points(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs['info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.使用模型来决策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "\n",
    "# 使用第一个可用的 GPU，即设备 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from base_module import *\n",
    "\n",
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "env.reset()\n",
    "\n",
    "config = GPT2Config(n_embd=512, n_layer=8, n_head=8, n_positions=128)\n",
    "model = Policy_Network(config).to(device)\n",
    "model.eval()\n",
    "\n",
    "collator = myCollator()\n",
    "for i in range(100):\n",
    "    while not env.is_over():\n",
    "        curr_pid = env.get_curr_player_id()\n",
    "        obs = env.get_observation(curr_pid)\n",
    "        input = {\n",
    "        \"tiles_features\": torch.tensor(obs['tiles_features'], dtype=torch.float32).to(device),\n",
    "        \"oya\": torch.tensor(obs['info']['oya'], dtype=torch.float32).unsqueeze(0).to(device),\n",
    "        \"riichi_sticks\": torch.tensor(obs['info']['riichi_sticks'],dtype=torch.float32).unsqueeze(0).to(device),\n",
    "        \"action_list\": torch.tensor(obs['action_list'],dtype=torch.long).unsqueeze(0).to(device),\n",
    "        \"attention_mask\": torch.tensor(obs['attention_mask'],dtype=torch.long).unsqueeze(0).to(device),\n",
    "        \"legal_action_mask\": torch.tensor(obs['legal_action_mask'], dtype=bool).to(device)\n",
    "        }\n",
    "        output = model.inference(**input)\n",
    "        action = output[\"action\"].item()\n",
    "        env.step(player_id=curr_pid, action_idx=action)\n",
    "        # print(\"player:\", curr_pid, \"action:\", action)\n",
    "    print(env.get_payoffs())\n",
    "    env.reset()\n",
    "\n",
    "\n",
    "\n",
    "# print(\"action:\", output[\"action\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_device: cuda:0\n",
      "training_device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "import torch\n",
    "\n",
    "# 使用第一个可用的 GPU，即设备 1\n",
    "inference_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "training_device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"inference_device:\", inference_device)\n",
    "print(\"training_device:\", training_device)\n",
    "\n",
    "import numpy as np\n",
    "from base_module import *\n",
    "\n",
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "env.reset()\n",
    "\n",
    "config = GPT2Config(n_embd=512, n_layer=8, n_head=8, n_positions=128)\n",
    "inference_model = Policy_Network(config).to(inference_device)\n",
    "inference_model.eval()\n",
    "\n",
    "training_model = Policy_Network(config).to(training_device)\n",
    "training_model.load_state_dict(inference_model.state_dict())\n",
    "training_model.train()\n",
    "\n",
    "collator = myCollator(device=training_device)\n",
    "inference_collator = inference_Collator(device=inference_device)\n",
    "optimizer = torch.optim.Adam(training_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "100%|██████████| 1/1 [00:42<00:00, 42.08s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "epochs = 1\n",
    "num_games = 4\n",
    "train_epochs = 1\n",
    "\n",
    "for epoch in tqdm(range(epochs)): \n",
    "   # 生成num_games局对局数据\n",
    "    dataset = []\n",
    "    while len(dataset) < num_games*4:\n",
    "        while not env.is_over():\n",
    "            curr_pid = env.get_curr_player_id()\n",
    "            obs = env.get_observation(curr_pid)\n",
    "            input = inference_collator(obs)\n",
    "            with torch.no_grad():\n",
    "                output = inference_model.inference(**input)\n",
    "                action = output[\"action\"].item()\n",
    "            env.step(player_id=curr_pid, action_idx=action)\n",
    "        payoffs = env.get_payoffs()\n",
    "        if not (payoffs == [0, 0, 0, 0]).all():\n",
    "            for player_id in range(4):\n",
    "                dataset.append(env.get_observation_with_return(player_id))\n",
    "            print(payoffs)\n",
    "            # break\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "    # 从dataset中每次取出batch_size个数据\n",
    "    \n",
    "    batch_size = 4\n",
    "\n",
    "    for train_epoch in tqdm(range(train_epochs)):\n",
    "        # 打乱数据\n",
    "        random.shuffle(dataset)\n",
    "        for batch_idx in range(0, len(dataset), batch_size):\n",
    "            batch = dataset[batch_idx:batch_idx+batch_size]\n",
    "            data = collator(batch)\n",
    "            # 移动数据到training_device\n",
    "            for key in data.keys():\n",
    "                if key == \"info\":\n",
    "                    for sub_key in data[key].keys():\n",
    "                        data[key][sub_key] = data[key][sub_key].to(training_device)\n",
    "                else:\n",
    "                    data[key] = data[key].to(training_device)\n",
    "            optimizer.zero_grad()\n",
    "            output = training_model.forward(**data)\n",
    "            loss = output[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print(\"loss:\", loss.item())\n",
    "\n",
    "    # 将training_model的参数复制到inference_model,并保存checkpoint\n",
    "    inference_model.load_state_dict(training_model.state_dict())\n",
    "    torch.save(inference_model.state_dict(), f\"./checkpoints/epoch_{epoch}.pth\")\n",
    "    # print(len(dataset))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_valid_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(env.get_curr_player_id(), 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 检查模型效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Policy_Network(\n",
       "  (my_model): my_model(\n",
       "    (tiles_cnn): Tiles_CNN(\n",
       "      (conv1): Conv2d(35, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (resnet_list): ModuleList(\n",
       "        (0-17): 18 x Rsidual_Block(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (squential): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (fc1): Linear(in_features=4608, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (pub_info_embedding): pub_info_embedding(\n",
       "      (oya_sticks_embedding): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (layernorm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (action_GPT2): action_GPT2(\n",
       "      (wte): Embedding(50257, 512)\n",
       "      (wpe): Embedding(128, 512)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-7): 8 x GPT2Block(\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (embeded): Embedding(189, 512)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=656, out_features=128, bias=True)\n",
       "  (fcList): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc2): Linear(in_features=128, out_features=47, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "\n",
    "# 使用第一个可用的 GPU，即设备 1\n",
    "inference_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# training_device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"inference_device:\", inference_device)\n",
    "# print(\"training_device:\", training_device)\n",
    "\n",
    "import numpy as np\n",
    "from base_module import *\n",
    "\n",
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "env.reset()\n",
    "\n",
    "config = GPT2Config(n_embd=512, n_layer=8, n_head=8, n_positions=128)\n",
    "inference_model = Policy_Network(config).to(inference_device)\n",
    "inference_collator = inference_Collator(device=inference_device)\n",
    "\n",
    "inference_model.load_state_dict(torch.load(\"./checkpoints/epoch_2.pth\", weights_only=True))\n",
    "inference_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aviable_action:\n",
      "['Discard 9m', 'Discard 8s', 'Discard 2m', 'Discard 4p', 'Discard 3p', 'Discard 2p', 'Discard 3s', 'Discard 4s']\n",
      "Pt: 25000\n",
      "Wind: North\n",
      "Hand: 2m 9m 9m 2p 2p 3p 4p 3s 4s 4s 8s \n",
      "Calls: 8m(8m)8m \n",
      "River: 2z4 3m8h 1s12h 5p16- \n",
      "Riichi: No\n",
      "Menzen: No\n",
      "curr_pid: 0\n",
      "valid_actions: [1, 8, 10, 11, 12, 20, 21, 25]\n",
      "model selected action: 8 prob: 12.632443010807037 %\n"
     ]
    }
   ],
   "source": [
    "curr_pid = env.get_curr_player_id()\n",
    "valid_actions = env.get_valid_actions()\n",
    "obs = env.get_observation(curr_pid)\n",
    "input = inference_collator(obs)\n",
    "output = inference_model.inference(**input)\n",
    "action = output[\"action\"].item()\n",
    "\n",
    "phase = env.t.get_phase()\n",
    "if phase < 4:\n",
    "    aviable_action = env.t.get_self_actions()\n",
    "elif phase < 16:\n",
    "    aviable_action = env.t.get_response_actions()\n",
    "print(\"aviable_action:\")\n",
    "ls = []\n",
    "for idx, av_action in enumerate(aviable_action):\n",
    "    ls.append(av_action.to_string())\n",
    "\n",
    "print(list(set(ls)))\n",
    "\n",
    "print(env.t.players[curr_pid].to_string())\n",
    "print(\"curr_pid:\", curr_pid)\n",
    "print(\"valid_actions:\", valid_actions)\n",
    "print(\"model selected action:\", action, \"prob:\", output[\"action_probs\"][0][action].item()*100, \"%\")\n",
    "\n",
    "env.step(player_id=curr_pid, action_idx=action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.检查环境bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from base_module import *\n",
    "\n",
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "\n",
    "flag = True\n",
    "# for i in range(1):\n",
    "while flag:\n",
    "    env.reset()\n",
    "    while not env.is_over():\n",
    "        curr_pid = env.get_curr_player_id()\n",
    "        phase = env.t.get_phase()\n",
    "        if phase < 4:\n",
    "            aviable_action = env.t.get_self_actions()\n",
    "        elif phase < 16:\n",
    "            aviable_action = env.t.get_response_actions()\n",
    "\n",
    "\n",
    "        ls = []\n",
    "        for action in aviable_action:\n",
    "            ls.append(action.action.name)\n",
    "        set_ls = set(ls)\n",
    "        if \"Riichi\" in set_ls:\n",
    "            flag = False\n",
    "            print(\"phase:\", phase)\n",
    "            print(\"aviable_action:\", set_ls)\n",
    "            break\n",
    "\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        action = np.random.choice(valid_actions)\n",
    "        env.step(player_id=curr_pid, action_idx=action)\n",
    "\n",
    "    # for i in range(4):\n",
    "    #     obs = env.get_observation_with_return(i)\n",
    "    #     print(\"---------------------------\"\"player:\", i, \"---------------------------\")\n",
    "    #     print(obs)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from base_module import *\n",
    "\n",
    "from pymahjong import *\n",
    "from pymahjong import MahjongPyWrapper as pm\n",
    "from pymahjong.myEnv_pymahjong import myMahjongEnv\n",
    "\n",
    "env = myMahjongEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"phase:\", env.t.get_phase())\n",
    "print(\"curr_pid:\", env.get_curr_player_id())\n",
    "phase = env.t.get_phase()\n",
    "if phase < 4:\n",
    "    aviable_action = env.t.get_self_actions()\n",
    "elif phase < 16:\n",
    "    aviable_action = env.t.get_response_actions()\n",
    "print(\"aviable_action:\")\n",
    "ls = []\n",
    "for idx, action in enumerate(aviable_action):\n",
    "    ls.append(action.to_string())\n",
    "    print(idx, action.to_string())\n",
    "    # print(action.action.name)\n",
    "\n",
    "print(\"aviable_action:\", ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_valid_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(env.get_curr_player_id(), 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.riichi_stage2)\n",
    "print(env.pass_riichi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.t.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 手操检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.make_selection(7)\n",
    "phase = env.t.get_phase()\n",
    "if phase < 4:\n",
    "    aviable_action = env.t.get_self_actions()\n",
    "elif phase < 16:\n",
    "    aviable_action = env.t.get_response_actions()\n",
    "print(\"after phase:\", phase)\n",
    "print(\"after aviable_action:\")\n",
    "for idx, action in enumerate(aviable_action):\n",
    "    print(idx, action.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.make_selection(0)\n",
    "phase = env.t.get_phase()\n",
    "if phase < 4:\n",
    "    aviable_action = env.t.get_self_actions()\n",
    "elif phase < 16:\n",
    "    aviable_action = env.t.get_response_actions()\n",
    "print(\"after phase:\", phase)\n",
    "print(\"after aviable_action:\")\n",
    "for idx, action in enumerate(aviable_action):\n",
    "    print(idx, action.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
