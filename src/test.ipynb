{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymahjong as mj\n",
    "import numpy as np\n",
    "\n",
    "env = mj.MahjongEnv()\n",
    "seed = \"mt19937ar-sha512-n288-base64,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\"\n",
    "env.reset(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_pid = env.get_curr_player_id()\n",
    "print(\"Current player id: \", curr_pid)\n",
    "valid_actions = env.get_valid_actions()\n",
    "print(\"Valid actions: \", valid_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(curr_pid, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.players[1].hand_to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用PaipuReplayer对天凤的牌谱进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymahjong as mj\n",
    "import numpy as np\n",
    "from pymahjong import MahjongPyWrapper as mp\n",
    "from pymahjong.extract_data import *\n",
    "import os\n",
    "\n",
    "seed_str = \"mt19937ar-sha512-n288-base64,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\"\n",
    "prefix = \"mt19937ar-sha512-n288-base64,\"\n",
    "path = os.getcwd() + \"/dataset\"\n",
    "filename = \"test.log\"\n",
    "seed = seed_str[len(prefix):]\n",
    "inst = mp.TenhouShuffle.instance()\n",
    "yama = inst.generate_yama()\n",
    "scores = [25000, 25000, 25000, 25000]\n",
    "riichi_sticks = 0\n",
    "honba = 0\n",
    "game_order = 0\n",
    "oya_id = 0\n",
    "\n",
    "# myReplay = myPaipuReplay(path=path, paipu_list=[filename])\n",
    "# myReplay._paipu_replay(path=path, paipu=filename)\n",
    "\n",
    "replayer = mp.PaipuReplayer()\n",
    "replayer.set_write_log(True)\n",
    "replayer.init(yama, scores, riichi_sticks, honba, game_order, oya_id)\n",
    "print(yama)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 牌局进程查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看当前局面和玩家手牌\n",
    "print(\"当前阶段：\", replayer.get_phase())\n",
    "print(\"正在响应的玩家：\", replayer.table.who_make_selection())\n",
    "print(\"玩家0:\", replayer.table.players[0].hand_to_string())\n",
    "print(\"玩家1:\", replayer.table.players[1].hand_to_string())\n",
    "print(\"玩家2:\", replayer.table.players[2].hand_to_string())\n",
    "print(\"玩家3:\", replayer.table.players[3].hand_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_actions = replayer.get_self_actions()\n",
    "response_actions = replayer.get_response_actions()\n",
    "print(\"可选主动动作：\")\n",
    "for i, action in enumerate(self_actions):\n",
    "    print(i, action.to_string())\n",
    "print(\"可选响应动作：\")\n",
    "for i, action in enumerate(response_actions):\n",
    "    print(i, action.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replayer.make_selection(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 天凤牌谱回放并保存记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymahjong as mj\n",
    "import numpy as np\n",
    "from pymahjong import tenhou_paipu_check as tp\n",
    "from pymahjong import extract_data as ed\n",
    "import os, gzip, pickle\n",
    "from multiprocessing import Process\n",
    "\n",
    "\n",
    "ps = []\n",
    "years = [2014, 2015]\n",
    "ouput_path = os.getcwd() + \"/../../../dataset/output\"\n",
    "for year in years:\n",
    "    path = os.getcwd() + f\"/../../../dataset/{year}\"\n",
    "    file_list = os.listdir(path)\n",
    "    # 把文件分成N份，每份开一个进程处理\n",
    "    # 每一份的文件数\n",
    "    N = 20\n",
    "    num_files = len(file_list) // N\n",
    "    for sep in range(N):\n",
    "        start = sep * num_files\n",
    "        end = (sep + 1) * num_files\n",
    "        if sep == N-1:\n",
    "            end = len(file_list)\n",
    "        p = Process(target=ed.multi_process, args=(path, file_list[start:end], year, ouput_path))\n",
    "        p.start()\n",
    "        ps.append(p)\n",
    "\n",
    "\n",
    "for p in ps:\n",
    "    p.join()\n",
    "\n",
    "print(\"All done!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ps:\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymahjong as mj\n",
    "import numpy as np\n",
    "from pymahjong import tenhou_paipu_check as tp\n",
    "from pymahjong import extract_data as ed\n",
    "import os, gzip, pickle\n",
    "from multiprocessing import Process\n",
    "\n",
    "path = os.getcwd() + f\"/../../../dataset/2013\"\n",
    "file_list = ['24701.gz']\n",
    "ouput_path = os.getcwd() + \"/../../../dataset/output\"\n",
    "\n",
    "ed.multi_process(path, file_list, 2013, ouput_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查回放后生成的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymahjong as mj\n",
    "import numpy as np\n",
    "from pymahjong import tenhou_paipu_check as tp\n",
    "from pymahjong import extract_data as ed\n",
    "import os, gzip, pickle\n",
    "from multiprocessing import Process\n",
    "\n",
    "# 定义打开gz文件的函数\n",
    "def open_gz_file(path):\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "ouput_path = os.getcwd() + \"/../../../dataset/output\"\n",
    "data = open_gz_file(ouput_path + \"/2013_24701.gz\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清理无用文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymahjong as mj\n",
    "import numpy as np\n",
    "from pymahjong import tenhou_paipu_check as tp\n",
    "from pymahjong import extract_data as ed\n",
    "import os, gzip, pickle\n",
    "from multiprocessing import Process\n",
    "\n",
    "# 定义打开gz文件的函数\n",
    "def open_gz_file(path):\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "ouput_path = os.getcwd() + \"/../../../dataset/output\"\n",
    "file_list = os.listdir(ouput_path)\n",
    "\n",
    "# 从文件列表中找出以.gz结尾的文件并去除后缀\n",
    "files = []\n",
    "for file in file_list:\n",
    "    if file.endswith(\".npz\"):\n",
    "        files.append(file[:-10])\n",
    "\n",
    "for file in files:\n",
    "    data = open_gz_file(ouput_path + \"/\" + file + \".gz\")\n",
    "    if len(data['action']) > 0 and len(data['action'][0]) > 0 and len(data['action'][0][0]) > 0:\n",
    "        if 'scores' in data['action'][0][0][0] and data['action'][0][0][0]['scores'] is not None:\n",
    "            # 确保数据结构有效后再执行逻辑\n",
    "            if len(data['action'][0][0][0]['dora_tiles']) > 1:\n",
    "                pass\n",
    "        else:\n",
    "            os.remove(ouput_path + \"/\" + file + \".gz\")\n",
    "            os.remove(ouput_path + \"/\" + file + \"_tehai.npz\")\n",
    "            print(\"Remove file: \", file)\n",
    "    else:\n",
    "        os.remove(ouput_path + \"/\" + file + \".gz\")\n",
    "        os.remove(ouput_path + \"/\" + file + \"_tehai.npz\")\n",
    "        print(\"Remove file: \", file)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查原始的牌谱文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymahjong as mj\n",
    "import numpy as np\n",
    "from pymahjong import tenhou_paipu_check as tp\n",
    "from pymahjong import extract_data as ed\n",
    "import os, gzip\n",
    "from multiprocessing import Process\n",
    "\n",
    "# 定义打开gz文件的函数\n",
    "def open_gz_file(path):\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "path = os.getcwd() + f\"/../../../dataset/2013\"\n",
    "file_list = ['7748.gz']\n",
    "\n",
    "data = open_gz_file(path + \"/24701.gz\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymahjong.extract_data import *\n",
    "from NN.model_predict_hand import *\n",
    "\n",
    "\n",
    "path = ouput_path = os.getcwd() + \"/../../../dataset/output\"\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "# 从文件列表中找出以.gz结尾的文件并去除后缀\n",
    "files = []\n",
    "for file in file_list:\n",
    "    if file.endswith(\".npz\"):\n",
    "        files.append(file[:-10])\n",
    "\n",
    "print(len(files))\n",
    "dataset = MahjongDataset(path, files[:10000])\n",
    "# print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = ouput_path = os.getcwd() + \"/../../../dataset/output\"\n",
    "data = np.load(path + \"/2014_20737_tehai.npz\")\n",
    "print(data['arr_0'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.__getitem__(0)['target'][0][0])\n",
    "# print(dataset.__getitem__(0)['mask'][0][0])\n",
    "print(dataset.__getitem__(-1)['target'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NN.model_predict_hand import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pymahjong.extract_data import *\n",
    "from NN.model_predict_hand import *\n",
    "\n",
    "# path = ouput_path = os.getcwd() + \"/../../../dataset/output\"\n",
    "# file_list = os.listdir(path)\n",
    "\n",
    "# # 从文件列表中找出以.gz结尾的文件并去除后缀\n",
    "# files = []\n",
    "# for file in file_list:\n",
    "#     if file.endswith(\".npz\"):\n",
    "#         files.append(file[:-10])\n",
    "\n",
    "# print(len(files))\n",
    "\n",
    "# dataset = MahjongDataset(path, files[:10])\n",
    "batch = [dataset.__getitem__(i) for i in range(10)]\n",
    "\n",
    "collator = MahjongCollator()\n",
    "# print(batch[0])\n",
    "\n",
    "collator_data = collator(batch)\n",
    "print('action_list shape:', collator_data['action'].shape)\n",
    "print('mask shape:', collator_data['mask'].shape)\n",
    "print('dora shape:', collator_data['info']['dora'].shape)\n",
    "print('tehai shape:', collator_data['info']['dora'][:44])\n",
    "print('scores shape:', collator_data['info']['scores'].shape)\n",
    "print('oya shape:', collator_data['info']['oya'].shape)\n",
    "print('hanba_riichi_sticks shape:', collator_data['info']['honba_riichi_sticks'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator_data['info']['dora']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from NN.model_predict_hand import *\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "\n",
    "path = ouput_path = os.getcwd() + \"/../../../dataset/output\"\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "# 从文件列表中找出以.gz结尾的文件并去除后缀\n",
    "files = []\n",
    "for file in file_list:\n",
    "    if file.endswith(\".npz\"):\n",
    "        files.append(file[:-10])\n",
    "\n",
    "# print(len(files))\n",
    "\n",
    "dataset = MahjongDataset(path, files[:100])\n",
    "batch = [dataset.__getitem__(i) for i in range(1)]\n",
    "\n",
    "collator = MahjongCollator(device=device)\n",
    "# print(batch[0])\n",
    "\n",
    "collator_data = collator(batch)\n",
    "\n",
    "config = GPT2Config(n_positions=256, n_embd=512, n_layer=12, n_head=16)\n",
    "my_model = myModel(config)\n",
    "my_model.to(device).double()\n",
    "output = my_model(collator_data)\n",
    "\n",
    "# output, mask = embedding(collator_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss', output['loss'])\n",
    "print('sum_loss', output['sum_loss'])\n",
    "print('sum_mask', output['sum_mask'])\n",
    "print('logits shape:', output['logits'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets_shape: (323474, 4, 256, 4, 37)\n",
      "inputs_len: 323474\n",
      "targets_shape: (32172, 4, 256, 4, 37)\n",
      "inputs_len: 32172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10109/50545 [3:36:47<11:15:45,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Evaluation Loss: 0.669074157064051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20218/50545 [7:20:57<8:28:08,  1.01s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Evaluation Loss: 0.653020809587854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30327/50545 [11:04:23<5:40:02,  1.01s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Evaluation Loss: 0.6360828426201822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40436/50545 [14:48:26<2:48:49,  1.00s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Evaluation Loss: 0.6241133248000211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50545/50545 [18:39:05<00:00,  1.18s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Evaluation Loss: 0.6143386785837102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from NN.model_predict_hand import *\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "import os\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path = ouput_path = os.getcwd() + \"/../../../dataset/output\"\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "# 从文件列表中找出以.gz结尾的文件并去除后缀\n",
    "files = []\n",
    "for file in file_list:\n",
    "    if file.endswith(\".npz\"):\n",
    "        files.append(file[:-10])\n",
    "\n",
    "# print(len(files))\n",
    "\n",
    "train_dataset = MahjongDataset(path, files[:30000])\n",
    "eval_dataset = MahjongDataset(path, files[-3000:])\n",
    "\n",
    "# train_dataset = MahjongDataset(path, files[:100])\n",
    "# eval_dataset = MahjongDataset(path, files[100:110])\n",
    "\n",
    "accelerator = Accelerator()\n",
    "config = GPT2Config(n_positions=256, n_embd=512, n_layer=12, n_head=16)\n",
    "my_model = myModel(config)\n",
    "my_model.to(torch.float32)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=MahjongCollator())\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False, collate_fn=MahjongCollator())\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader, eval_dataloader, my_model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, my_model, optimizer\n",
    ")\n",
    "\n",
    "save_directory = os.getcwd() + \"/checkpoints\"\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "train_progress_bar = tqdm(range(num_training_steps))\n",
    "eval_progress_bar = tqdm(range(len(eval_dataloader)))\n",
    "\n",
    "my_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    my_model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        output = my_model(**batch)\n",
    "        if output == None:\n",
    "            print(\"output is None\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        loss = output['loss']\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_progress_bar.update(1)\n",
    "        if step % 2000 == 0:\n",
    "            # 保存checkpoint\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(my_model)\n",
    "            accelerator.save(unwrapped_model.state_dict(), os.path.join(save_directory, f\"checkpoint_{epoch}_{step}.pt\"))\n",
    "\n",
    "    my_model.eval()\n",
    "    eval_losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            output = my_model(**batch)\n",
    "        if output is None:\n",
    "            print(\"output is None\")\n",
    "            continue\n",
    "        loss = output['loss']\n",
    "        eval_losses.append(loss.item())\n",
    "        eval_progress_bar.update(1)\n",
    "\n",
    "    avg_eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "    print(f\"Epoch {epoch} Evaluation Loss: {avg_eval_loss}\")\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(my_model)\n",
    "    accelerator.save(unwrapped_model.state_dict(), os.path.join(save_directory, f\"final_checkpoint_epoch_{epoch}.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取checkpoints继续需训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets_shape: (323474, 4, 256, 4, 37)\n",
      "inputs_len: 323474\n",
      "targets_shape: (32172, 4, 256, 4, 37)\n",
      "inputs_len: 32172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10109/101090 [3:37:02<25:35:47,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Evaluation Loss: 0.6084323630892259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20218/101090 [7:23:19<22:45:09,  1.01s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Evaluation Loss: 0.6053338471511249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30327/101090 [11:13:00<19:27:31,  1.01it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Evaluation Loss: 0.6032724669629016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40436/101090 [14:54:47<16:37:41,  1.01it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Evaluation Loss: 0.6023699058808579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50545/101090 [18:38:40<13:55:57,  1.01it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Evaluation Loss: 0.6019612393132735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60654/101090 [22:21:22<11:18:33,  1.01s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Evaluation Loss: 0.6021683341110677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70763/101090 [26:06:50<8:28:55,  1.01s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Evaluation Loss: 0.6027102124975405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80872/101090 [29:50:00<5:39:06,  1.01s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Evaluation Loss: 0.6039231027926413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90981/101090 [33:30:56<2:47:32,  1.01it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Evaluation Loss: 0.6055919645913316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101090/101090 [37:12:24<00:00,  1.01it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Evaluation Loss: 0.6081691847051351\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from NN.model_predict_hand import *\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "import os\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path = ouput_path = os.getcwd() + \"/../../../dataset/output\"\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "# 从文件列表中找出以.gz结尾的文件并去除后缀\n",
    "files = []\n",
    "for file in file_list:\n",
    "    if file.endswith(\".npz\"):\n",
    "        files.append(file[:-10])\n",
    "\n",
    "# print(len(files))\n",
    "\n",
    "train_dataset = MahjongDataset(path, files[:30000])\n",
    "eval_dataset = MahjongDataset(path, files[-3000:])\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "checkpoints_path = os.getcwd() + \"/checkpoints\"\n",
    "checkpoints_name = \"final_checkpoint_epoch_4.pt\"\n",
    "config = GPT2Config(n_positions=256, n_embd=512, n_layer=12, n_head=16)\n",
    "my_model = myModel(config)\n",
    "my_model.to(torch.float32)\n",
    "my_model.load_state_dict(torch.load(os.path.join(checkpoints_path, checkpoints_name)))\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=1e-4)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=MahjongCollator())\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False, collate_fn=MahjongCollator())\n",
    "\n",
    "train_dataloader, eval_dataloader, my_model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, my_model, optimizer\n",
    ")\n",
    "\n",
    "save_directory = os.getcwd() + \"/checkpoints\"\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "train_progress_bar = tqdm(range(num_training_steps))\n",
    "eval_progress_bar = tqdm(range(len(eval_dataloader)))\n",
    "\n",
    "my_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    my_model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        output = my_model(**batch)\n",
    "        if output == None:\n",
    "            print(\"output is None\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        loss = output['loss']\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_progress_bar.update(1)\n",
    "        if step % 2000 == 0:\n",
    "            # 保存checkpoint\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(my_model)\n",
    "            accelerator.save(unwrapped_model.state_dict(), os.path.join(save_directory, f\"checkpoint_{epoch+5}_{step}.pt\"))\n",
    "\n",
    "    my_model.eval()\n",
    "    eval_losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            output = my_model(**batch)\n",
    "        if output is None:\n",
    "            print(\"output is None\")\n",
    "            continue\n",
    "        loss = output['loss']\n",
    "        eval_losses.append(loss.item())\n",
    "        eval_progress_bar.update(1)\n",
    "\n",
    "    avg_eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "    print(f\"Epoch {epoch} Evaluation Loss: {avg_eval_loss}\")\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(my_model)\n",
    "    accelerator.save(unwrapped_model.state_dict(), os.path.join(save_directory, f\"final_checkpoint_epoch_{epoch+5}.pt\"))\n",
    "\n",
    "                         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from NN.model_predict_hand import *\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "import os\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path = ouput_path = os.getcwd() + \"/../../../dataset/output\"\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "# 从文件列表中找出以.gz结尾的文件并去除后缀\n",
    "files = []\n",
    "for file in file_list:\n",
    "    if file.endswith(\".npz\"):\n",
    "        files.append(file[:-10])\n",
    "\n",
    "# print(len(files))\n",
    "\n",
    "train_dataset = MahjongDataset(path, files[:100])\n",
    "eval_dataset = MahjongDataset(path, files[-10:])\n",
    "collator = MahjongCollator()\n",
    "config = GPT2Config(n_positions=256, n_embd=512, n_layer=12, n_head=16)\n",
    "my_model = myModel(config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./mahjong_pred_checkpoints',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=my_model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
