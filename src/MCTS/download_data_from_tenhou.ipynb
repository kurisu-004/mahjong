{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# 从压缩包中提取数据到csv文件\n",
    "import os\n",
    "import tarfile\n",
    "import gzip\n",
    "import zipfile\n",
    "import csv\n",
    "import re\n",
    "\n",
    "tenhou_files_path = '../dataset/Tenhou'\n",
    "files = os.listdir(tenhou_files_path)\n",
    "pattern = re.compile(r'(\\d{4})/scc')\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    # 判断是否为压缩文件\n",
    "    if not file.endswith('.zip'):\n",
    "        continue\n",
    "\n",
    "    with zipfile.ZipFile(os.path.join(tenhou_files_path, file), 'r') as zip_ref:\n",
    "\n",
    "        namelist = zip_ref.namelist()\n",
    "        extracted_data_filename = file[5:9] + \".csv\"\n",
    "        with open(os.path.join(tenhou_files_path, extracted_data_filename), 'w') as extracted_data_file:\n",
    "            writer = csv.writer(extracted_data_file)\n",
    "            writer.writerow(['date', 'time', 'game_rule', 'log_url'])\n",
    "\n",
    "            for name in namelist:\n",
    "                if name.endswith('.html.gz') and re.match(pattern, name):\n",
    "                    year = name[-16:-12]\n",
    "                    month = name[-12:-10]\n",
    "                    day = name[-10:-8]\n",
    "                    date = year + \"/\" + month + \"/\"+ day\n",
    "                    print(date)\n",
    "\n",
    "                    zip_ref.extract(name, os.path.join(tenhou_files_path, 'temp'))\n",
    "                    with gzip.open(os.path.join(tenhou_files_path, 'temp', name), 'r') as html_file:\n",
    "                        data_b = html_file.readlines()\n",
    "                        data = [line.decode('utf-8').strip() for line in data_b]\n",
    "                        # print(data[0])\n",
    "                        # print(len(data))\n",
    "                        # break\n",
    "                        for i in range(len(data)):\n",
    "                            split_data = data[i].split('|')\n",
    "                            time = split_data[0].strip()\n",
    "                            game_rule = split_data[2].strip()\n",
    "                            raw_url = split_data[3].strip()\n",
    "                            # print(split_data)\n",
    "                            # break\n",
    "\n",
    "                            try:\n",
    "                                # 提取URL链接部分\n",
    "                                start_index = raw_url.find('http')  # 找到URL的起始位置\n",
    "                                end_index = raw_url.find('\">')  # 找到URL的结束位置\n",
    "\n",
    "                                if start_index != -1 and end_index != -1:\n",
    "                                    url_part = raw_url[start_index:end_index]  # 提取URL部分\n",
    "\n",
    "                                    # 替换\"?log=\"为\"log/?\"\n",
    "                                    modified_url = url_part.replace('?log=', 'log/?')\n",
    "\n",
    "                                    # print(modified_url)  # 输出修改后的URL\n",
    "                            \n",
    "                            except:\n",
    "                                modified_url = 'None'\n",
    "                            \n",
    "                            series = [date, time, game_rule, modified_url]\n",
    "                            writer.writerow(series)\n",
    "\n",
    "\n",
    "                if name.endswith('.html') and name.startswith(''):\n",
    "\n",
    "                    year = name[-13:-9]\n",
    "                    month = name[-9:-7]\n",
    "                    day = name[-7:-5]\n",
    "                    date = year + \"/\" + month + \"/\"+ day\n",
    "\n",
    "                    with zip_ref.open(name, 'r') as html_file:\n",
    "                        data_b = html_file.readlines()\n",
    "                        data = [line.decode('utf-8').strip() for line in data_b]\n",
    "                        # print(data[0])\n",
    "                        # print(len(data))\n",
    "                        # break\n",
    "                        for i in range(len(data)):\n",
    "                            split_data = data[i].split('|')\n",
    "                            time = split_data[0].strip()\n",
    "                            game_rule = split_data[2].strip()\n",
    "                            raw_url = split_data[3].strip()\n",
    "                            # print(split_data)\n",
    "                            # break\n",
    "\n",
    "                            try:\n",
    "                                # 提取URL链接部分\n",
    "                                start_index = raw_url.find('http')  # 找到URL的起始位置\n",
    "                                end_index = raw_url.find('\">')  # 找到URL的结束位置\n",
    "\n",
    "                                if start_index != -1 and end_index != -1:\n",
    "                                    url_part = raw_url[start_index:end_index]  # 提取URL部分\n",
    "\n",
    "                                    # 替换\"?log=\"为\"log/?\"\n",
    "                                    modified_url = url_part.replace('?log=', 'log/?')\n",
    "\n",
    "                                    # print(modified_url)  # 输出修改后的URL\n",
    "                            \n",
    "                            except:\n",
    "                                modified_url = 'None'\n",
    "                            \n",
    "                            series = [date, time, game_rule, modified_url]\n",
    "                            writer.writerow(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选四鳳南喰赤的数据\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# 读取csv文件\n",
    "tenhou_files_path = '../dataset/Tenhou'\n",
    "files = os.listdir(tenhou_files_path)\n",
    "pattern = re.compile(r'(\\d{4})\\.csv')\n",
    "\n",
    "for file in files:\n",
    "    if pattern.match(file):\n",
    "        data = pd.read_csv(os.path.join(tenhou_files_path, file))\n",
    "        nan_4_kuitan_aka = data[data['game_rule'].str.startswith('四鳳南喰赤')]\n",
    "        print(file[:-4],\"年总对局：\")\n",
    "        print(data.shape)\n",
    "        print(\"四鳳南喰赤总对局：\")\n",
    "        print(nan_4_kuitan_aka.shape)\n",
    "        nan_4_kuitan_aka.to_csv(os.path.join(tenhou_files_path, file[:-4] + '_nan_4_kuitan_aka.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据URL下载牌谱数据并保存\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import time\n",
    "\n",
    "years = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "csv_path = '../../dataset/Tenhou'\n",
    "HEADER = {\n",
    "    'Host': 'e.mjv.jp',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:65.0) Gecko/20100101 Firefox/65.0',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive'\n",
    "}\n",
    "for year in years:\n",
    "    data = pd.read_csv(os.path.join(csv_path, f'{year}_nan_4_kuitan_aka.csv'))\n",
    "    save_path = os.path.join(csv_path, f'{year}_nan_4_kuitan_aka')\n",
    "\n",
    "    # 读取第一条数据\n",
    "    for index, url in data['log_url'].items():\n",
    "        req = urllib.request.Request(url, headers=HEADER)\n",
    "        opener = urllib.request.build_opener()\n",
    "        response = opener.open(req)\n",
    "        if response.info().get('Content-Encoding') == 'gzip':\n",
    "            file_name = str(index + 2) + '.gz'\n",
    "            with open(os.path.join(save_path, file_name), 'wb') as f:\n",
    "                f.write(response.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_contents = []\n",
    "HEADER = {\n",
    "    'Host': 'e.mjv.jp',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:65.0) Gecko/20100101 Firefox/65.0',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive'\n",
    "}\n",
    "\n",
    "data = pd.read_csv(f'../../dataset/Tenhou/{year[0]}_nan_4_kuitan_aka.csv')\n",
    "urls = data['log_url'][0:10]\n",
    "\n",
    "for index, url in urls.items():\n",
    "    req = urllib.request.Request(url, headers=HEADER)\n",
    "    opener = urllib.request.build_opener()\n",
    "    response = opener.open(req)\n",
    "    if response.info().get('Content-Encoding') == 'gzip':\n",
    "        file_name = str(index + 2) + '.gz'\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(response.read())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mahjong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
